{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9921bc53-5dac-45f4-8a91-df29a57d3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e69f591-e790-4939-ba3a-b50bc0fb0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " [[0.34849442 0.70189894]\n",
      " [0.36651102 0.68131782]\n",
      " [0.33805986 0.6537384 ]\n",
      " [0.33371996 0.69768299]]\n",
      "Attention Weights:\n",
      " [[0.30399137 0.26374837 0.16815237 0.26410789]\n",
      " [0.28623393 0.29672212 0.18259781 0.23444614]\n",
      " [0.25963374 0.25978996 0.23217766 0.24839865]\n",
      " [0.29723294 0.24312372 0.18105339 0.27858995]]\n",
      "\n",
      "\n",
      "----------分界线----------\n",
      "\n",
      "\n",
      "Attention Output:\n",
      " [[0.37454012 0.95071431]]\n",
      "Attention Weights:\n",
      " [[1.]]\n",
      "\n",
      "\n",
      "----------分界线----------\n",
      "\n",
      "\n",
      "Attention Output:\n",
      " [[0.39296863 0.68747658]\n",
      " [0.42702208 0.65494911]\n",
      " [0.40233016 0.64356457]\n",
      " [0.37516149 0.69187854]\n",
      " [0.41509897 0.66579213]\n",
      " [0.36971587 0.69921133]\n",
      " [0.44548969 0.62722922]]\n",
      "Attention Weights:\n",
      " [[0.17237952 0.14955956 0.09535147 0.14976343 0.15549089 0.15899318\n",
      "  0.11846195]\n",
      " [0.15936433 0.16520376 0.10166362 0.13053083 0.16170176 0.13376868\n",
      "  0.147767  ]\n",
      " [0.14742525 0.14751396 0.13183514 0.14104574 0.14716478 0.1420799\n",
      "  0.14293523]\n",
      " [0.17007298 0.13911236 0.10359649 0.1594057  0.14795681 0.16960074\n",
      "  0.1102549 ]\n",
      " [0.16368772 0.15975298 0.10020066 0.13715656 0.15961747 0.14218395\n",
      "  0.13740067]\n",
      " [0.17327641 0.13681649 0.10014958 0.16276429 0.14719747 0.17467043\n",
      "  0.10512533]\n",
      " [0.14580198 0.17068095 0.11378352 0.11949587 0.16064315 0.11872194\n",
      "  0.1708726 ]]\n"
     ]
    }
   ],
   "source": [
    "#此函数将各个单词attention值标准化并总和为1，成为标准attention值的格式\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "#此函数计算attention值\n",
    "def attention(query, key, value):\n",
    "    scores = np.dot(query, key.T)\n",
    "\n",
    "    scale = np.sqrt(key.shape[-1])\n",
    "    scores /= scale\n",
    "\n",
    "    weights = softmax(scores)\n",
    "\n",
    "    output = np.dot(weights, value)\n",
    "    return output, weights\n",
    "\n",
    "#在此函数中，embedding_dim可调\n",
    "#在对sentence做embedding处理后，运用attention并显示结果\n",
    "def sentence_to_attention(sentence, embedding_dim=2):\n",
    "    words = sentence.split()\n",
    "\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.rand(len(words), embedding_dim)\n",
    "\n",
    "    query = embeddings\n",
    "    key = embeddings\n",
    "    value = embeddings\n",
    "    \n",
    "    output, weights = attention(query, key, value)\n",
    "    \n",
    "    print(\"Attention Output:\\n\", output)\n",
    "    print(\"Attention Weights:\\n\", weights)\n",
    "\n",
    "#运用示范\n",
    "sentence1 = \"Today is July 19th.\"\n",
    "sentence_to_attention(sentence1)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"----------分界线----------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "sentence2 = \"今天是7月19日。\"\n",
    "sentence_to_attention(sentence2)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"----------分界线----------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "sentence2 = \"今 天 是 7 月 19 日。\"\n",
    "sentence_to_attention(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b876c0-c6b9-4c26-b227-46cb9ed98ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output:\n",
      " [[-1.48293084  0.75647753 -0.3313181   1.05777141]\n",
      " [-0.51012977 -0.37033002 -0.82764469  1.70810449]\n",
      " [-0.11888339 -0.71121139 -0.83705269  1.66714747]]\n",
      "\n",
      "\n",
      "Encoder Attention Weights:\n",
      " [[0.63398533 0.20356514 0.16244953]\n",
      " [0.28802469 0.31861361 0.3933617 ]\n",
      " [0.18720687 0.32038272 0.49241041]]\n",
      "\n",
      "\n",
      "Decoder Output:\n",
      " [[-1.02147432  0.2144661  -0.73336813  1.54037635]\n",
      " [-0.72770163 -0.09804882 -0.83610401  1.66185446]]\n",
      "\n",
      "\n",
      "Decoder Self-Attention Weights:\n",
      " [[1.         0.        ]\n",
      " [0.44322455 0.55677545]]\n",
      "\n",
      "\n",
      "Encoder-Decoder Attention Weights:\n",
      " [[0.31823617 0.36909888 0.31266495]\n",
      " [0.20718191 0.39893009 0.39388801]]\n"
     ]
    }
   ],
   "source": [
    "def initialize_random_embeddings(vocab_size, embedding_dim):\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.rand(vocab_size, embedding_dim)\n",
    "    return embeddings\n",
    "\n",
    "#在encode-decode中所需要的positional encoding，此函数会帮助系统识别sentence用词的先后顺序，从而更准确识别句子信息的attention\n",
    "def get_positional_encoding(seq_length, embedding_dim):\n",
    "    positional_encoding = np.zeros((seq_length, embedding_dim))\n",
    "    for pos in range(seq_length):\n",
    "        for i in range(embedding_dim):\n",
    "            if i % 2 == 0:\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** (i / embedding_dim)))\n",
    "            else:\n",
    "                positional_encoding[pos, i] = np.cos(pos / (10000 ** ((i - 1) / embedding_dim)))\n",
    "    return positional_encoding\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def attention(query, key, value, mask=None):\n",
    "    scores = np.dot(query, key.T)\n",
    "    scale = np.sqrt(key.shape[-1])\n",
    "    scores /= scale\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "    weights = softmax(scores)\n",
    "    output = np.dot(weights, value)\n",
    "    return output, weights\n",
    "\n",
    "#此函数将x内信息标准化\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)\n",
    "\n",
    "#RELU函数\n",
    "def feed_forward(x, hidden_dim):\n",
    "    W1 = np.random.rand(x.shape[-1], hidden_dim)\n",
    "    b1 = np.random.rand(hidden_dim)\n",
    "    W2 = np.random.rand(hidden_dim, x.shape[-1])\n",
    "    b2 = np.random.rand(x.shape[-1])\n",
    "    hidden = np.maximum(0, np.dot(x, W1) + b1)\n",
    "    return np.dot(hidden, W2) + b2\n",
    "\n",
    "#学习资料中:\n",
    "def create_mask(seq_length):\n",
    "    mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype(np.float32)\n",
    "    return mask\n",
    "def encode(sentence, vocab, embedding_dim=16, hidden_dim=32):\n",
    "    embeddings = initialize_random_embeddings(len(vocab), embedding_dim)\n",
    "    positional_encodings = get_positional_encoding(len(sentence), embedding_dim)\n",
    "    sentence_embeddings = np.array([embeddings[vocab.index(word)] for word in sentence])\n",
    "    sentence_embeddings += positional_encodings[:len(sentence)]\n",
    "    attention_output, attention_weights = attention(sentence_embeddings, sentence_embeddings, sentence_embeddings)\n",
    "    attention_output = layer_norm(sentence_embeddings + attention_output)\n",
    "    ff_output = feed_forward(attention_output, hidden_dim)\n",
    "    encoder_output = layer_norm(attention_output + ff_output)\n",
    "    return encoder_output, attention_weights\n",
    "def decode(target_sentence, encoder_output, vocab, embedding_dim=16, hidden_dim=32):\n",
    "    embeddings = initialize_random_embeddings(len(vocab), embedding_dim)\n",
    "    positional_encodings = get_positional_encoding(len(target_sentence), embedding_dim)\n",
    "    target_embeddings = np.array([embeddings[vocab.index(word)] for word in target_sentence])\n",
    "    target_embeddings += positional_encodings[:len(target_sentence)]\n",
    "    mask = create_mask(len(target_sentence))\n",
    "    attention_output, attention_weights = attention(target_embeddings, target_embeddings, target_embeddings, mask)\n",
    "    attention_output = layer_norm(target_embeddings + attention_output)\n",
    "    enc_dec_attention_output, enc_dec_attention_weights = attention(attention_output, encoder_output, encoder_output)\n",
    "    enc_dec_attention_output = layer_norm(attention_output + enc_dec_attention_output)\n",
    "    ff_output = feed_forward(enc_dec_attention_output, hidden_dim)\n",
    "    decoder_output = layer_norm(enc_dec_attention_output + ff_output)\n",
    "    return decoder_output, attention_weights, enc_dec_attention_weights\n",
    "\n",
    "\n",
    "vocab = ['today', 'is', 'july', 'the', '19th']\n",
    "input_sentence = ['today', 'is', 'july']\n",
    "target_sentence = ['the', '19th']\n",
    "\n",
    "encoder_output, encoder_attention_weights = encode(input_sentence, vocab, embedding_dim=4, hidden_dim=8)\n",
    "\n",
    "decoder_output, decoder_attention_weights, enc_dec_attention_weights = decode(target_sentence, encoder_output, vocab, embedding_dim=4, hidden_dim=8)\n",
    "\n",
    "print(\"Encoder Output:\\n\", encoder_output)\n",
    "print(\"\\n\")\n",
    "print(\"Encoder Attention Weights:\\n\", encoder_attention_weights)\n",
    "print(\"\\n\")\n",
    "print(\"Decoder Output:\\n\", decoder_output)\n",
    "print(\"\\n\")\n",
    "print(\"Decoder Self-Attention Weights:\\n\", decoder_attention_weights)\n",
    "print(\"\\n\")\n",
    "print(\"Encoder-Decoder Attention Weights:\\n\", enc_dec_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a94498-0c3b-4903-9b47-f8f5872a396c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
